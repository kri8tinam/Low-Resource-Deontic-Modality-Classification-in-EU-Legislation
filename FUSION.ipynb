{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "collapsed_sections": [
        "2Eaa4WK5zGjv",
        "hsmYgOKvKHDG",
        "UIb5LLyjkcKm",
        "rXn-3ustt6Nm",
        "soXwQ-AE6V6f",
        "OLy54J1U6ekR",
        "8mo2fWPn6kNO",
        "Vm2eBgCl6xjl",
        "6E-I_X-P616m",
        "tbqVe33nxk5g",
        "FMbr6EbZxmN2"
      ],
      "authorship_tag": "ABX9TyMMWxVLsX/NP7sqrex0R5J6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kri8tinam/Low-Resource-Deontic-Modality-Classification-in-EU-Legislation/blob/main/FUSION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Eaa4WK5zGjv"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqvZPwUHusCp",
        "outputId": "8d6adbe4-2706-44ac-9e79-c1a1e7fae739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n",
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Collecting keras_lr_finder\n",
            "  Downloading keras_lr_finder-0.1-py2.py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from keras_lr_finder) (2.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from keras_lr_finder) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->keras_lr_finder) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->keras_lr_finder) (1.16.0)\n",
            "Installing collected packages: keras_lr_finder\n",
            "Successfully installed keras_lr_finder-0.1\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from torch_optimizer) (2.0.1+cu118)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.5.0->torch_optimizer) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.5.0->torch_optimizer) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
            "Installing collected packages: pytorch-ranger, torch_optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n",
            "Collecting tensorflow-model-optimization\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.23.5)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.16.0)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.7.5\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.7.22)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-0d7e7f60ed9c>:53: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner.tuners import RandomSearch\n"
          ]
        }
      ],
      "source": [
        "#loading and visualizing\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "import os\n",
        "import random as rn\n",
        "\n",
        "#preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "!pip install keras_lr_finder\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "from keras_lr_finder import LRFinder\n",
        "# from clr_callback import CyclicLR\n",
        "\n",
        "\n",
        "!pip install tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "!pip install keras-tuner\n",
        "import keras_tuner\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "import time\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R5IK3nNw2wQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0d937e-396c-47c3-f84f-4dd96fc31a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "hsmYgOKvKHDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/TRAIN.csv', delimiter= ',')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/TEST.csv', delimiter= ',')\n",
        "\n",
        "# Samples\n",
        "train100 = pd.read_csv('/content/drive/MyDrive/train100.csv', delimiter= ',')\n",
        "train200 = pd.read_csv('/content/drive/MyDrive/train200.csv', delimiter= ',')\n",
        "train300 = pd.read_csv('/content/drive/MyDrive/train300.csv', delimiter= ',')\n",
        "train350 = pd.read_csv('/content/drive/MyDrive/train350.csv', delimiter= ',')\n",
        "train400 = pd.read_csv('/content/drive/MyDrive/train400.csv', delimiter= ',')\n",
        "train450 = pd.read_csv('/content/drive/MyDrive/train450.csv', delimiter= ',')\n",
        "\n",
        "# Filtered & Augmented\n",
        "filtered_150 = pd.read_csv('/content/drive/MyDrive/train_filtered_150.csv', delimiter= ',')\n",
        "filtered_312 = pd.read_csv('/content/drive/MyDrive/train_filtered_312.csv', delimiter= ',')\n",
        "aug_train150 = pd.read_csv('/content/drive/MyDrive/aug_train150.csv', delimiter= ',')\n",
        "aug_train312 = pd.read_csv('/content/drive/MyDrive/aug_train312.csv', delimiter= ',')"
      ],
      "metadata": {
        "id": "eJrTfYhNKJOI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "Eace58MxKm6I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "acf308bf-9fee-4cfb-e2ec-fbf7ac008c1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  text      category\n",
              "0    Organizations must ensure that their product r...   obligations\n",
              "1    For a period of thirty years from 1 October 20...         right\n",
              "2    Member States may adopt measures to limit acce...         power\n",
              "3    For a period of seventeen years from 1 October...         right\n",
              "4    Organizations must ensure that their product r...   obligations\n",
              "..                                                 ...           ...\n",
              "321  Member States may adopt measures to limit acce...         power\n",
              "322  Organizations must ensure that their product a...   obligations\n",
              "323  The Union shall ensure the right to protection...  prohibitions\n",
              "324  For a period of twenty years from 1 November 2...         power\n",
              "325  EU citizens can receive public maternity assis...    permission\n",
              "\n",
              "[326 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-a69d99c8-2eee-4559-b7eb-23f62adbf68a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Organizations must ensure that their product r...</td>\n",
              "      <td>obligations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>For a period of thirty years from 1 October 20...</td>\n",
              "      <td>right</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Member States may adopt measures to limit acce...</td>\n",
              "      <td>power</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>For a period of seventeen years from 1 October...</td>\n",
              "      <td>right</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Organizations must ensure that their product r...</td>\n",
              "      <td>obligations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>Member States may adopt measures to limit acce...</td>\n",
              "      <td>power</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>Organizations must ensure that their product a...</td>\n",
              "      <td>obligations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>The Union shall ensure the right to protection...</td>\n",
              "      <td>prohibitions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>For a period of twenty years from 1 November 2...</td>\n",
              "      <td>power</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>EU citizens can receive public maternity assis...</td>\n",
              "      <td>permission</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>326 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a69d99c8-2eee-4559-b7eb-23f62adbf68a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-9ce324a9-4bd8-4d2a-bbb1-3e22fa57ee53\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ce324a9-4bd8-4d2a-bbb1-3e22fa57ee53')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-9ce324a9-4bd8-4d2a-bbb1-3e22fa57ee53 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a69d99c8-2eee-4559-b7eb-23f62adbf68a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a69d99c8-2eee-4559-b7eb-23f62adbf68a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data['category'].value_counts())\n",
        "print(test_data['category'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O56roGjEu0e",
        "outputId": "45b5f7c6-2ab4-45de-f04a-d16dab8652d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "permission     117\n",
            "prohibition    108\n",
            "obligation      85\n",
            "omission        84\n",
            "power           74\n",
            "right           47\n",
            "Name: category, dtype: int64\n",
            "obligations     71\n",
            "permission      66\n",
            "right           60\n",
            "prohibitions    58\n",
            "power           54\n",
            "omission        17\n",
            "Name: category, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIb5LLyjkcKm"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_list = [r'\\d', '\\+', ':', '\\\\n', '#', '$', '%', '&','\"', ',', '-',\n",
        "             ';', '<', '=', '>', '@', '!', '^', '_', '`', '{', '|', '}', '~']   # list of special characters/punctuation\n",
        "\n",
        "def char_search(text, col, characters):\n",
        "  for char in characters:\n",
        "    count = text[col].str.contains(char).sum()\n",
        "    print(f'{char} special character is present in {count} entries')\n",
        "\n",
        "char_search(test_data, 'text', char_list)"
      ],
      "metadata": {
        "id": "Y8sVNxVkCreu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleanse(text, col, characters):\n",
        "  text[col] = text[col].str.lower() # convert to lowercase\n",
        "  for char in characters:\n",
        "    text[col] = text[col].str.replace(char, '', regex = True) # remove special characters\n",
        "\n",
        "text_cleanse(test_data, 'text', char_list)\n",
        "\n",
        "char_search(test_data, 'text', char_list)"
      ],
      "metadata": {
        "id": "HsYAE6HXajVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mzNU02q2Ugth"
      },
      "outputs": [],
      "source": [
        "# CLEANING THE DATASETS\n",
        "\n",
        "def cleaning(text):\n",
        "  text = text.replace('$', '')\n",
        "  text = text.replace('^', '')\n",
        "  text = re.sub(r'\\n\\n', '. ', text)\n",
        "  text = re.sub(r\"\\(\\d+\\)\", \"\", text)\n",
        "  text = text.replace('   ', \" \")\n",
        "  text = re.sub(r\"\\b\\d\\. \\b\", \"\", text)\n",
        "  text = text.strip()\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Label Encoding"
      ],
      "metadata": {
        "id": "OMGAcvSUokE-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3EgMZ11cpzBU"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "train_data['category'] = le.fit_transform(train_data[\"category\"])\n",
        "test_data['category'] = le.fit_transform(test_data[\"category\"])\n",
        "\n",
        "filtered_150['category'] = le.fit_transform(filtered_150[\"category\"])\n",
        "filtered_312['category'] = le.fit_transform(filtered_312[\"category\"])\n",
        "aug_train150['category'] = le.fit_transform(aug_train150[\"category\"])\n",
        "aug_train312['category'] = le.fit_transform(aug_train312[\"category\"])\n",
        "\n",
        "train100['category'] = le.fit_transform(train100[\"category\"])\n",
        "train200['category'] = le.fit_transform(train200[\"category\"])\n",
        "train300['category'] = le.fit_transform(train300[\"category\"])\n",
        "train350['category'] = le.fit_transform(train350[\"category\"])\n",
        "train400['category'] = le.fit_transform(train400[\"category\"])\n",
        "train450['category'] = le.fit_transform(train450[\"category\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fusion model"
      ],
      "metadata": {
        "id": "kKXuCtATcL1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark"
      ],
      "metadata": {
        "id": "wPsRX152O8qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN + A + BILSTM + CNN\n",
        "\n",
        "*   optimizer - RMSProp\n",
        "*   batch size = 50\n"
      ],
      "metadata": {
        "id": "jdtSSXuoPC1p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6MCIQkZexR2P"
      },
      "outputs": [],
      "source": [
        "def reproduceResult():\n",
        "  seed_value= 0\n",
        "\n",
        "\n",
        "  with tf.device(\"/cpu:0\"):\n",
        "    ...\n",
        "\n",
        "\n",
        "  os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "  np.random.seed(0)\n",
        "  rn.seed(0)\n",
        "\n",
        "\n",
        "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                          inter_op_parallelism_threads=1)\n",
        "\n",
        "\n",
        "  tf.compat.v1.set_random_seed(seed_value)\n",
        "  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "  tf.compat.v1.keras.backend.set_session(sess)\n",
        "  tf.compat.v1.keras.backend.clear_session()\n",
        "\n",
        "reproduceResult()\n",
        "import tempfile\n",
        "import zipfile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFRnbsRHzYzJ"
      },
      "outputs": [],
      "source": [
        "reproduceResult()\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def lemmatization(text):\n",
        "    text = text.lower()\n",
        "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "train_data['lemmatized text'] = train_data.text.apply(lambda x: lemmatization(x))\n",
        "test_data['lemmatized text'] = test_data.text.apply(lambda x: lemmatization(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mJWrRxa0d2U"
      },
      "outputs": [],
      "source": [
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(train_data['lemmatized text'])\n",
        "tokenized_test = tokenizer.texts_to_sequences(test_data['lemmatized text'])\n",
        "\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = train_data['category']\n",
        "\n",
        "X_test1 = pad_sequences(tokenized_test, maxlen=MAX_LEN,padding='pre')\n",
        "y_test1 = test_data['category']\n",
        "\n",
        "print(X_train1.shape)\n",
        "print(X_test1.shape)\n",
        "print(y_train1.shape)\n",
        "print(y_test1.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    with open('/content/drive/MyDrive/cc.en.300.vec') as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word]\n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "45Y6NL4kzio1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "def benchmark(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "\n",
        "  # SEARCH VALUES\n",
        "  # Adjust these values later based on the reported 'best so far' values\n",
        "  att_size = hp.Int(\"attention size\",min_value =16, max_value = 128, step = 16)\n",
        "  cnn_1_size = hp.Int(\"cnn1 size\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn1 dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_size = hp.Int(\"lstm size\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_dropout = hp.Float(\"lstm dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "  cnn_2_size = hp.Int(\"cnn2 size\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_2_dropout = hp.Float(\"cnn2 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  lr = hp.Float(\"learning rate\",min_value = 2e-5 ,max_value = 1e-3,step = 2e-5)\n",
        "\n",
        "\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(cnn_1_size,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(att_size))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,att_size))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # BILSTM LAYER\n",
        "  lstm = keras.layers.Bidirectional(keras.layers.LSTM(lstm_size,\n",
        "                                                      return_sequences=True,input_shape =(128,)))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(cnn_2_size,3)(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer = keras.optimizers.RMSprop(learning_rate = lr), metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# EARLY STOPPING\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    benchmark,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 50, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "4k4mkxUKzred"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minimal Sample"
      ],
      "metadata": {
        "id": "38p-IdRKQWOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train100['lemmatized text'] = train100.text.apply(lambda x: lemmatization(x))\n",
        "train200['lemmatized text'] = train200.text.apply(lambda x: lemmatization(x))\n",
        "train300['lemmatized text'] = train300.text.apply(lambda x: lemmatization(x))\n",
        "train350['lemmatized text'] = train350.text.apply(lambda x: lemmatization(x))\n",
        "train400['lemmatized text'] = train400.text.apply(lambda x: lemmatization(x))\n",
        "train450['lemmatized text'] = train450.text.apply(lambda x: lemmatization(x))"
      ],
      "metadata": {
        "id": "3c8QEwtmwTjH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original train data"
      ],
      "metadata": {
        "id": "Um6QiaDBEkSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(train_data['lemmatized text'])\n",
        "tokenized_test = tokenizer.texts_to_sequences(test_data['lemmatized text'])\n",
        "\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = train_data['category']\n",
        "\n",
        "X_test1 = pad_sequences(tokenized_test, maxlen=MAX_LEN,padding='pre')\n",
        "y_test1 = test_data['category']\n",
        "\n",
        "\n",
        "print(X_train1.shape)\n",
        "print(X_test1.shape)\n",
        "print(y_train1.shape)\n",
        "print(y_test1.shape)\n",
        "\n",
        "\n",
        "def create_embedding_matrix(word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    with open('/content/drive/MyDrive/cc.en.300.vec') as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word]\n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "\n",
        "  # SEARCH VALUES\n",
        "  unit_attention = hp.Int(\"attention_unit\",min_value =16, max_value = 128, step = 16)\n",
        "  cnn_1_unit = hp.Int(\"cnn_1_unit\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn_1_dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_unit = hp.Int(\"lstm_unit\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_dropout = hp.Float(\"lstm_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "  cnn_2_unit = hp.Int(\"cnn_2_unit\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_2_dropout = hp.Float(\"cnn_2_dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  lr = hp.Float(\"learning rate\",min_value = 1e-5 ,max_value = 1e-3,step = 2e-5)\n",
        "\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn = keras.layers.Conv1D(cnn_1_unit,3)(embedded)\n",
        "  cnn = keras.layers.Activation(activation='relu')(cnn)\n",
        "  cnn = keras.layers.BatchNormalization()(cnn)\n",
        "  cnn = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention_vec = keras.layers.TimeDistributed(keras.layers.Dense(unit_attention))(cnn)\n",
        "  attention_vec = keras.layers.Reshape((128,unit_attention))(attention_vec)\n",
        "  attention_vec = keras.layers.Activation('relu', name = 'cnn_attention_vec')(attention_vec)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn, attention_vec])\n",
        "\n",
        "  # BILSTM LAYER\n",
        "  lstm = keras.layers.Bidirectional(keras.layers.LSTM(lstm_unit,\n",
        "                                                      return_sequences=True,input_shape =(128,)))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(cnn_2_unit,3)(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss = 'sparse_categorical_crossentropy', optimizer = keras.optimizers.RMSprop(learning_rate = lr), metrics = ['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "# Different batch sizes to test\n",
        "batch_sizes = (4,8,16,32,50)\n",
        "\n",
        "# Dictionary to store validation accuracy and hyperparameters for each batch size\n",
        "results = {}\n",
        "\n",
        "for batch in batch_sizes:\n",
        "  print('Batch size =', batch)\n",
        "\n",
        "  LOG_DIR = f\"{int(time.time())}\"\n",
        "  seed_value= 0\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "  tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "  tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = batch, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "  # Get the best hyperparameters and validation accuracy for the current batch size\n",
        "  best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "  best_val_accuracy = tuner.oracle.get_best_trials(1)[0].score\n",
        "\n",
        "  # Store the results in the dictionary\n",
        "  results[batch] = {'best_val_accuracy': best_val_accuracy, 'best_hyperparameters': best_hyperparameters}\n",
        "\n",
        "\n",
        "print('The results for the original train data using different batch sizes:')\n",
        "results\n"
      ],
      "metadata": {
        "id": "rNOovJoKETEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 100 samples"
      ],
      "metadata": {
        "id": "rXn-3ustt6Nm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YMp1YPlv-dT"
      },
      "outputs": [],
      "source": [
        "# MAX_FEATURES = 6000\n",
        "\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train100['lemmatized text'])\n",
        "\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(train100['lemmatized text'])\n",
        "tokenized_test = tokenizer.texts_to_sequences(test_data['lemmatized text'])\n",
        "\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = train100['category']\n",
        "\n",
        "X_test1 = pad_sequences(tokenized_test, maxlen=MAX_LEN,padding='pre')\n",
        "y_test1 = test_data['category']\n",
        "\n",
        "\n",
        "\n",
        "print(X_train1.shape)\n",
        "print(X_test1.shape)\n",
        "print(y_train1.shape)\n",
        "print(y_test1.shape)\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "# Different batch sizes to test\n",
        "batch_sizes = (4,8,16,32,50)\n",
        "\n",
        "# Dictionary to store validation accuracy and hyperparameters for each batch size\n",
        "results100 = {}\n",
        "\n",
        "for batch in batch_sizes:\n",
        "  print('Batch size =', batch)\n",
        "\n",
        "  LOG_DIR = f\"{int(time.time())}\"\n",
        "  seed_value= 0\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "  tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "  tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = batch, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "  # Get the best hyperparameters and validation accuracy for the current batch size\n",
        "  best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "  best_val_accuracy = tuner.oracle.get_best_trials(1)[0].score\n",
        "\n",
        "  # Store the results in the dictionary\n",
        "  results100[batch] = {'best_val_accuracy': best_val_accuracy, 'best_hyperparameters': best_hyperparameters}\n",
        "\n",
        "\n",
        "results100"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 200 samples"
      ],
      "metadata": {
        "id": "soXwQ-AE6V6f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUgSx4f-6V6o"
      },
      "outputs": [],
      "source": [
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train200['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(train200['lemmatized text'])\n",
        "tokenized_test = tokenizer.texts_to_sequences(test_data['lemmatized text'])\n",
        "\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = train200['category']\n",
        "\n",
        "X_test1 = pad_sequences(tokenized_test, maxlen=MAX_LEN,padding='pre')\n",
        "y_test1 = test_data['category']\n",
        "\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "# Different batch sizes to test\n",
        "batch_sizes = (4,8,16,32,50)\n",
        "\n",
        "# Dictionary to store validation accuracy and hyperparameters for each batch size\n",
        "results200 = {}\n",
        "\n",
        "for batch in batch_sizes:\n",
        "  print('Batch size =', batch)\n",
        "\n",
        "  LOG_DIR = f\"{int(time.time())}\"\n",
        "  seed_value= 0\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "  tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "  tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = batch, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "  # Get the best hyperparameters and validation accuracy for the current batch size\n",
        "  best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "  best_val_accuracy = tuner.oracle.get_best_trials(1)[0].score\n",
        "\n",
        "  # Store the results in the dictionary\n",
        "  results200[batch] = {'best_val_accuracy': best_val_accuracy, 'best_hyperparameters': best_hyperparameters}\n",
        "\n",
        "\n",
        "results200\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 300 samples"
      ],
      "metadata": {
        "id": "OLy54J1U6ekR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMO0SoCY6ekf"
      },
      "outputs": [],
      "source": [
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train300['lemmatized text'])\n",
        "\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(train300['lemmatized text'])\n",
        "tokenized_test = tokenizer.texts_to_sequences(test_data['lemmatized text'])\n",
        "\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = train300['category']\n",
        "\n",
        "X_test1 = pad_sequences(tokenized_test, maxlen=MAX_LEN,padding='pre')\n",
        "y_test1 = test_data['category']\n",
        "\n",
        "\n",
        "print(X_train1.shape)\n",
        "print(X_test1.shape)\n",
        "print(y_train1.shape)\n",
        "print(y_test1.shape)\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "# Different batch sizes to test\n",
        "batch_sizes = (4,8,16,32,50)\n",
        "\n",
        "# Dictionary to store validation accuracy and hyperparameters for each batch size\n",
        "results300 = {}\n",
        "\n",
        "for batch in batch_sizes:\n",
        "  print('Batch size =', batch)\n",
        "\n",
        "  LOG_DIR = f\"{int(time.time())}\"\n",
        "  seed_value= 0\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "  tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "  tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = batch, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "  # Get the best hyperparameters and validation accuracy for the current batch size\n",
        "  best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "  best_val_accuracy = tuner.oracle.get_best_trials(1)[0].score\n",
        "\n",
        "  # Store the results in the dictionary\n",
        "  results300[batch] = {'best_val_accuracy': best_val_accuracy, 'best_hyperparameters': best_hyperparameters}\n",
        "\n",
        "\n",
        "results300"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 350 samples"
      ],
      "metadata": {
        "id": "8mo2fWPn6kNO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyQe6xIx6kNR"
      },
      "outputs": [],
      "source": [
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train350['lemmatized text'])\n",
        "\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(train350['lemmatized text'])\n",
        "tokenized_test = tokenizer.texts_to_sequences(test_data['lemmatized text'])\n",
        "\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = train350['category']\n",
        "\n",
        "X_test1 = pad_sequences(tokenized_test, maxlen=MAX_LEN,padding='pre')\n",
        "y_test1 = test_data['category']\n",
        "\n",
        "\n",
        "# X_train1, X_test1 = train_test_split(X_train1, test_size=0.2,  random_state = 42)\n",
        "# y_train1, y_test1 = train_test_split(y_train1, test_size=0.2,  random_state = 42)\n",
        "\n",
        "\n",
        "print(X_train1.shape)\n",
        "print(X_test1.shape)\n",
        "print(y_train1.shape)\n",
        "print(y_test1.shape)\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "# Different batch sizes to test\n",
        "batch_sizes = (4,8,16,32,50)\n",
        "\n",
        "# Dictionary to store validation accuracy and hyperparameters for each batch size\n",
        "results350 = {}\n",
        "\n",
        "for batch in batch_sizes:\n",
        "  print('Batch size =', batch)\n",
        "\n",
        "  LOG_DIR = f\"{int(time.time())}\"\n",
        "  seed_value= 0\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "  tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "  tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = batch, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "  # Get the best hyperparameters and validation accuracy for the current batch size\n",
        "  best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "  best_val_accuracy = tuner.oracle.get_best_trials(1)[0].score\n",
        "\n",
        "  # Store the results in the dictionary\n",
        "  results350[batch] = {'best_val_accuracy': best_val_accuracy, 'best_hyperparameters': best_hyperparameters}\n",
        "\n",
        "\n",
        "results350"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 400 samples"
      ],
      "metadata": {
        "id": "Vm2eBgCl6xjl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSi01cqQ6xjt"
      },
      "outputs": [],
      "source": [
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train400['lemmatized text'])\n",
        "\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(train400['lemmatized text'])\n",
        "tokenized_test = tokenizer.texts_to_sequences(test_data['lemmatized text'])\n",
        "\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = train400['category']\n",
        "\n",
        "X_test1 = pad_sequences(tokenized_test, maxlen=MAX_LEN,padding='pre')\n",
        "y_test1 = test_data['category']\n",
        "\n",
        "\n",
        "print(X_train1.shape)\n",
        "print(X_test1.shape)\n",
        "print(y_train1.shape)\n",
        "print(y_test1.shape)\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "# Different batch sizes to test\n",
        "batch_sizes = (4,8,16,32,50)\n",
        "\n",
        "# Dictionary to store validation accuracy and hyperparameters for each batch size\n",
        "results400 = {}\n",
        "\n",
        "for batch in batch_sizes:\n",
        "  print('Batch size =', batch)\n",
        "\n",
        "  LOG_DIR = f\"{int(time.time())}\"\n",
        "  seed_value= 0\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "  tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "  tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = batch, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "  # Get the best hyperparameters and validation accuracy for the current batch size\n",
        "  best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "  best_val_accuracy = tuner.oracle.get_best_trials(1)[0].score\n",
        "\n",
        "  # Store the results in the dictionary\n",
        "  results400[batch] = {'best_val_accuracy': best_val_accuracy, 'best_hyperparameters': best_hyperparameters}\n",
        "\n",
        "\n",
        "results400"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 450 samples"
      ],
      "metadata": {
        "id": "6E-I_X-P616m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVFOIRSb616o"
      },
      "outputs": [],
      "source": [
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train450['lemmatized text'])\n",
        "\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(train450['lemmatized text'])\n",
        "tokenized_test = tokenizer.texts_to_sequences(test_data['lemmatized text'])\n",
        "\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = train450['category']\n",
        "\n",
        "X_test1 = pad_sequences(tokenized_test, maxlen=MAX_LEN,padding='pre')\n",
        "y_test1 = test_data['category']\n",
        "\n",
        "\n",
        "print(X_train1.shape)\n",
        "print(X_test1.shape)\n",
        "print(y_train1.shape)\n",
        "print(y_test1.shape)\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "# Different batch sizes to test\n",
        "batch_sizes = (4,8,16,32,50)\n",
        "\n",
        "# Dictionary to store validation accuracy and hyperparameters for each batch size\n",
        "results450 = {}\n",
        "\n",
        "for batch in batch_sizes:\n",
        "  print('Batch size =', batch)\n",
        "\n",
        "  LOG_DIR = f\"{int(time.time())}\"\n",
        "  seed_value= 0\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "  tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "  tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = batch, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "  # Get the best hyperparameters and validation accuracy for the current batch size\n",
        "  best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
        "  best_val_accuracy = tuner.oracle.get_best_trials(1)[0].score\n",
        "\n",
        "  # Store the results in the dictionary\n",
        "  results450[batch] = {'best_val_accuracy': best_val_accuracy, 'best_hyperparameters': best_hyperparameters}\n",
        "\n",
        "\n",
        "results450"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters of the best-performing model"
      ],
      "metadata": {
        "id": "j95WN4UjnN8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the hyperparameters of the best-perfroming model based on the batch size\n",
        "\n",
        "best_hyperparameters = results[8]['best_hyperparameters']\n",
        "\n",
        "print('cnn1 size =', best_hyperparameters.get('cnn_1_unit'))\n",
        "print('cnn1 dropout =', best_hyperparameters.get('cnn_1_dropout'))\n",
        "print('attention vector =', best_hyperparameters.get('attention_unit'))\n",
        "print('bilstm size =', best_hyperparameters.get('lstm_unit'))\n",
        "print('bilstm dropout =', best_hyperparameters.get('lstm_dropout'))\n",
        "print('cnn2 size =', best_hyperparameters.get('cnn_2_unit'))\n",
        "print('cnn2 dropout =', best_hyperparameters.get('cnn_2_dropout'))\n",
        "print('learning rate =', best_hyperparameters.get('learning rate'))\n"
      ],
      "metadata": {
        "id": "jxggWBV5nW4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity filtering and Data augmentation"
      ],
      "metadata": {
        "id": "NvUqnwmtve-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model with the filtered and augmented data to see if there will be a performance improvement. Use the layer sizes, dropout ratios and batch size which resulted in the highest accuracy in the previous section."
      ],
      "metadata": {
        "id": "BsQNo8TQY4I-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline model compiled with the best-performing layer sizes, dropout ratios, learning rate and batch size (according to the previous section) and optimized with **RMSprop optimizer**"
      ],
      "metadata": {
        "id": "ZaoMQPrNMweL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjusted_baseline(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(16,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(0.1,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(112))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,112))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # BILSTM LAYER\n",
        "  lstm = keras.layers.Bidirectional(keras.layers.LSTM(224,\n",
        "                                                      return_sequences=True,input_shape =(128,)))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(0.1,seed=seed_value)(lstm)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(192,3)(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(0.5,seed=seed_value)(cnn_2)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer = keras.optimizers.RMSprop(learning_rate = 0.00035000000000000004), metrics=['accuracy'])\n",
        "  # use the learning rate value from the hyperparameters of the best-perfroming model\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "e4mmB8DgMu1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same model optimized with **ADAM optimizer**"
      ],
      "metadata": {
        "id": "6pT9msamNR0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjusted_baseline_adam(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(16,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(0.1,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(112))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,112))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # BILSTM LAYER\n",
        "  lstm = keras.layers.Bidirectional(keras.layers.LSTM(224,\n",
        "                                                      return_sequences=True,input_shape =(128,)))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(0.1,seed=seed_value)(lstm)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(192,3)(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(0.5,seed=seed_value)(cnn_2)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer = keras.optimizers.Adam(learning_rate = 0.00035000000000000004), metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "DEJhoRnONWls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original train data with Adam"
      ],
      "metadata": {
        "id": "BRWYKccUrJ_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(train_data['lemmatized text'])\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = train_data['category']\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# Adam\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    adjusted_baseline_adam,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "2Quw1aZtrL2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# RMSprop\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    adjusted_baseline,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "tkIylDmBSFGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filtered 150"
      ],
      "metadata": {
        "id": "PtiumO2IwgMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_150['lemmatized text'] = filtered_150.text.apply(lambda x: lemmatization(x))\n",
        "\n",
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(filtered_150['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(filtered_150['lemmatized text'])\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = filtered_150['category']\n",
        "\n",
        "\n",
        "print(X_train1.shape)\n",
        "print(y_train1.shape)\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n"
      ],
      "metadata": {
        "id": "_gkPkpajvui0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# RMSprop\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    adjusted_baseline,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "0-qCvab2xEzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# Adam\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    adjusted_baseline_adam,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "HbhfBI3INqzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filtered & augmented 150"
      ],
      "metadata": {
        "id": "FBFXAMeNxNj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug_train150['lemmatized text'] = aug_train150.text.apply(lambda x: lemmatization(x))\n",
        "\n",
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(aug_train150['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(aug_train150['lemmatized text'])\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = aug_train150['category']\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n"
      ],
      "metadata": {
        "id": "1N4ESlBkxROY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# RMSprop\n",
        "\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    adjusted_baseline,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "0_l-PV5EOBSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# Adam\n",
        "\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    adjusted_baseline_adam,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "101WKu-hOBSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filtered 312"
      ],
      "metadata": {
        "id": "tbqVe33nxk5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "filtered_312['lemmatized text'] = filtered_312.text.apply(lambda x: lemmatization(x))\n",
        "\n",
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(filtered_312['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(filtered_312['lemmatized text'])\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = filtered_312['category']\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n"
      ],
      "metadata": {
        "id": "udv5qAUwxl8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# RMSprop\n",
        "\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    adjusted_baseline,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "arYvVuCPOFaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# Adam\n",
        "\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    adjusted_baseline_adam,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "Rjh7Ile1OFaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filtered & augmented 312"
      ],
      "metadata": {
        "id": "FMbr6EbZxmN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug_train312['lemmatized text'] = aug_train312.text.apply(lambda x: lemmatization(x))\n",
        "\n",
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(aug_train312['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(aug_train312['lemmatized text'])\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = aug_train312['category']\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n"
      ],
      "metadata": {
        "id": "-3Hi5Vt4xmg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# RMSprop\n",
        "\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    adjusted_baseline,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "JTVqMJvuOIPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# Adam\n",
        "\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    adjusted_baseline_adam,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "71NqPxqGOIPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fusion architectures"
      ],
      "metadata": {
        "id": "QTFqIFTsQj-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN + att + BiLSTM"
      ],
      "metadata": {
        "id": "8jNvr4_6mHbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "import time\n",
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "\n",
        "  # SEARCH VALUES\n",
        "  att_size = hp.Int(\"attention size\",min_value =16, max_value = 128, step = 16)\n",
        "  cnn_1_size = hp.Int(\"cnn1 size\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn1 dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_size = hp.Int(\"lstm size\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_dropout = hp.Float(\"lstm dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "  cnn_2_size = hp.Int(\"cnn2 size\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_2_dropout = hp.Float(\"cnn2 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  lr = hp.Float(\"learning rate\",min_value = 2e-5 ,max_value = 1e-3,step = 2e-5)\n",
        "\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(cnn_1_size,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(att_size))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,att_size))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # BILSTM LAYER\n",
        "  lstm = keras.layers.Bidirectional(keras.layers.LSTM(lstm_size,\n",
        "                                                      return_sequences=True,input_shape =(128,)))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(lstm)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# EARLY STOPPING\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "JnuV_CvBmGEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN + att + BiLSTM + CNN"
      ],
      "metadata": {
        "id": "y1caVVBKXcZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "\n",
        "  # SEARCH VALUES\n",
        "  att_size = hp.Int(\"attention size\",min_value =16, max_value = 128, step = 16)\n",
        "  cnn_1_size = hp.Int(\"cnn1 size\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn1 dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_size = hp.Int(\"lstm size\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_dropout = hp.Float(\"lstm dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "  cnn_2_size = hp.Int(\"cnn2 size\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_2_dropout = hp.Float(\"cnn2 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  lr = hp.Float(\"learning rate\",min_value = 2e-5 ,max_value = 1e-3,step = 2e-5)\n",
        "\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(cnn_1_size,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(att_size))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,att_size))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # BILSTM LAYER\n",
        "  lstm = keras.layers.Bidirectional(keras.layers.LSTM(lstm_size,\n",
        "                                                      return_sequences=True,input_shape =(128,)))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(cnn_2_size,3)(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# EARLY STOPPING\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "K4Mw06fCXcZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN + att + BiLSTM + CNN + CNN"
      ],
      "metadata": {
        "id": "sE-R9N4PjU5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "\n",
        "  # SEARCH VALUES\n",
        "  att_size = hp.Int(\"attention size\",min_value =16, max_value = 128, step = 16)\n",
        "  cnn_1_size = hp.Int(\"cnn1 size\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn1 dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_size = hp.Int(\"lstm size\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_dropout = hp.Float(\"lstm dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "  cnn_2_size = hp.Int(\"cnn2 size\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_2_dropout = hp.Float(\"cnn2 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  cnn_3_size = hp.Int(\"cnn3 size\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_3_dropout = hp.Float(\"cnn3 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  lr = hp.Float(\"learning rate\",min_value = 2e-5 ,max_value = 1e-3,step = 2e-5)\n",
        "\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(cnn_1_size,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(att_size))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,att_size))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # BILSTM LAYER\n",
        "  lstm = keras.layers.Bidirectional(keras.layers.LSTM(lstm_size,\n",
        "                                                      return_sequences=True,input_shape =(128,)))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(cnn_2_size,3)(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
        "\n",
        "  # 3ND CNN LAYER\n",
        "  cnn_3 = keras.layers.Conv1D(cnn_3_size,3)(lstm)\n",
        "  cnn_3 = keras.layers.Activation(activation='relu')(cnn_3)\n",
        "  cnn_3 = keras.layers.BatchNormalization()(cnn_3)\n",
        "  cnn_3 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_3)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_3)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# EARLY STOPPING\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "JhbGnMYcjVHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN + A + BiLSTM + CNN + BiLSTM"
      ],
      "metadata": {
        "id": "WimGetZpWGPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "\n",
        "  # SEARCH VALUES\n",
        "  att_size = hp.Int(\"attention size\",min_value =16, max_value = 128, step = 16)\n",
        "  cnn_1_size = hp.Int(\"cnn1 size\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn1 dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_1_size = hp.Int(\"lstm1 size\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_1_dropout = hp.Float(\"lstm1 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  cnn_2_size = hp.Int(\"cnn2 size\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_2_dropout = hp.Float(\"cnn2 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  lstm_2_size = hp.Int(\"lstm2 size\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_2_dropout = hp.Float(\"lstm2 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  lr = hp.Float(\"learning rate\",min_value = 2e-5 ,max_value = 1e-3,step = 2e-5)\n",
        "\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(cnn_1_size,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(att_size))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,att_size))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # 1ST BILSTM LAYER\n",
        "  lstm_1 = keras.layers.Bidirectional(keras.layers.LSTM(lstm_1_size,\n",
        "                                                      return_sequences=True,input_shape =(128,)))(attention_output)\n",
        "  lstm_1 = keras.layers.Activation(activation='relu')(lstm_1)\n",
        "  lstm_1 = keras.layers.BatchNormalization()(lstm_1)\n",
        "  lstm_1 = keras.layers.Dropout(lstm_1_dropout,seed=seed_value)(lstm_1)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(cnn_2_size,3)(lstm_1)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
        "\n",
        "  # 2ND BILSTM LAYER\n",
        "  lstm_2 = keras.layers.Bidirectional(keras.layers.LSTM(lstm_2_size,\n",
        "                                                      return_sequences=True,input_shape =(128,)))(attention_output)\n",
        "  lstm_2 = keras.layers.Activation(activation='relu')(lstm_2)\n",
        "  lstm_2 = keras.layers.BatchNormalization()(lstm_2)\n",
        "  lstm_2 = keras.layers.Dropout(lstm_2_dropout,seed=seed_value)(lstm_2)\n",
        "\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(lstm_2)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# EARLY STOPPING\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "AYoCtrlbWNcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN + att + LSTM"
      ],
      "metadata": {
        "id": "61dKtnx8mr82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "\n",
        "  # SEARCH VALUES\n",
        "  att_size = hp.Int(\"attention size\",min_value =16, max_value = 128, step = 16)\n",
        "  cnn_1_size = hp.Int(\"cnn1 size\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn1 dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_size = hp.Int(\"lstm size\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_dropout = hp.Float(\"lstm dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "\n",
        "  lr = hp.Float(\"learning rate\",min_value = 2e-5 ,max_value = 1e-3,step = 2e-5)\n",
        "\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(cnn_1_size,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(att_size))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,att_size))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # LSTM LAYER\n",
        "  lstm = keras.layers.LSTM(lstm_size,return_sequences=True,input_shape =(128,))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(lstm)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# EARLY STOPPING\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "Fiej7bJAmsII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN + att + LSTM + CNN"
      ],
      "metadata": {
        "id": "3kptxjAZcQ92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "\n",
        "  # SEARCH VALUES\n",
        "  att_size = hp.Int(\"attention size\",min_value =16, max_value = 128, step = 16)\n",
        "  cnn_1_size = hp.Int(\"cnn1 size\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn1 dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_size = hp.Int(\"lstm size\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_dropout = hp.Float(\"lstm dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  cnn_2_size = hp.Int(\"cnn2 size\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_2_dropout = hp.Float(\"cnn2 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  lr = hp.Float(\"learning rate\",min_value = 2e-5 ,max_value = 1e-3,step = 2e-5)\n",
        "\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(cnn_1_size,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(att_size))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,att_size))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # LSTM LAYER\n",
        "  lstm = keras.layers.LSTM(lstm_size,return_sequences=True,input_shape =(128,))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(cnn_2_size,3)(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_2)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# EARLY STOPPING\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "K1YvW1bgccs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN + A + LSTM + CNN + CNN"
      ],
      "metadata": {
        "id": "-ZNOyTVbaiAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "\n",
        "  # SEARCH VALUES\n",
        "  att_size = hp.Int(\"attention size\",min_value =16, max_value = 128, step = 16)\n",
        "  cnn_1_size = hp.Int(\"cnn1 size\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn1 dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_size = hp.Int(\"lstm size\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_dropout = hp.Float(\"lstm dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "  cnn_2_size = hp.Int(\"cnn2 size\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_2_dropout = hp.Float(\"cnn2 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  cnn_3_size = hp.Int(\"cnn3 size\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_3_dropout = hp.Float(\"cnn3 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  lr = hp.Float(\"learning rate\",min_value = 2e-5 ,max_value = 1e-3,step = 2e-5)\n",
        "\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(cnn_1_size,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(att_size))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,att_size))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # LSTM LAYER\n",
        "  lstm = keras.layers.LSTM(lstm_size, return_sequences=True,input_shape =(128,))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(lstm_dropout,seed=seed_value)(lstm)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(cnn_2_size,3)(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
        "\n",
        "  # 3ND CNN LAYER\n",
        "  cnn_3 = keras.layers.Conv1D(cnn_3_size,3)(lstm)\n",
        "  cnn_3 = keras.layers.Activation(activation='relu')(cnn_3)\n",
        "  cnn_3 = keras.layers.BatchNormalization()(cnn_3)\n",
        "  cnn_3 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_3)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_3)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# EARLY STOPPING\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "YpCy3xGlaiMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN + att + LSTM + CNN + LSTM"
      ],
      "metadata": {
        "id": "3IB9ztz1crkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "\n",
        "  # SEARCH VALUES\n",
        "  att_size = hp.Int(\"attention size\",min_value =16, max_value = 128, step = 16)\n",
        "  cnn_1_size = hp.Int(\"cnn1 size\",min_value =16, max_value = 96, step = 16)\n",
        "  cnn_1_dropout = hp.Float(\"cnn1 dropout\",min_value = 0.1,max_value = 0.3,step = 0.1)\n",
        "\n",
        "  lstm_1_size = hp.Int(\"lstm1 size\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_1_dropout = hp.Float(\"lstm1 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  cnn_2_size = hp.Int(\"cnn2 size\",min_value =32, max_value = 256, step = 32)\n",
        "  cnn_2_dropout = hp.Float(\"cnn2 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  lstm_2_size = hp.Int(\"lstm2 size\",min_value =32, max_value = 256, step = 32)\n",
        "  lstm_2_dropout = hp.Float(\"lstm2 dropout\",min_value = 0.1,max_value = 0.5,step = 0.1)\n",
        "\n",
        "  lr = hp.Float(\"learning rate\",min_value = 2e-5 ,max_value = 1e-3,step = 2e-5)\n",
        "\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(cnn_1_size,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(cnn_1_dropout,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(att_size))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,att_size))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # LSTM LAYER\n",
        "  lstm_1 = keras.layers.LSTM(lstm_1_size,return_sequences=True,input_shape =(128,))(attention_output)\n",
        "  lstm_1 = keras.layers.Activation(activation='relu')(lstm_1)\n",
        "  lstm_1 = keras.layers.BatchNormalization()(lstm_1)\n",
        "  lstm_1 = keras.layers.Dropout(lstm_1_dropout,seed=seed_value)(lstm_1)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(cnn_2_size,3)(lstm_1)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(cnn_2_dropout,seed=seed_value)(cnn_2)\n",
        "\n",
        "  # 2ND LSTM LAYER\n",
        "  lstm_2 = keras.layers.LSTM(lstm_1_size,return_sequences=True,input_shape =(128,))(attention_output)\n",
        "  lstm_2 = keras.layers.Activation(activation='relu')(lstm_2)\n",
        "  lstm_2 = keras.layers.BatchNormalization()(lstm_2)\n",
        "  lstm_2 = keras.layers.Dropout(lstm_2_dropout,seed=seed_value)(lstm_2)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(lstm_2)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# EARLY STOPPING\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "Yad0Y1B4cyHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Focal loss"
      ],
      "metadata": {
        "id": "QtbG-q4dQpgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incorporating gamma into the hyperparameter search for the best-performing fusion architecture - CNN + A + LSTM + CNN + CNN, to evaluate whether it improves the performance of the model for the differently class distributed datasets."
      ],
      "metadata": {
        "id": "ickj9EPyWSW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### original train data"
      ],
      "metadata": {
        "id": "-ERsd95rX5T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install focal_loss\n",
        "from focal_loss import SparseCategoricalFocalLoss"
      ],
      "metadata": {
        "id": "IMlfOT-HU3NC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c326994-f071-4ca5-a951-2817f3a2585c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting focal_loss\n",
            "  Downloading focal_loss-0.0.7-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.10/dist-packages (from focal_loss) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2->focal_loss) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2->focal_loss) (0.41.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.2->focal_loss) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.2->focal_loss) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2->focal_loss) (3.2.2)\n",
            "Installing collected packages: focal_loss\n",
            "Successfully installed focal_loss-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "def focal_loss_model(hp):\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  print('Ya it comes here')\n",
        "\n",
        "\n",
        "  lr = hp.Float(\"learning rate\",min_value = 2e-5 ,max_value = 1e-3,step = 2e-5)\n",
        "  gamma = hp.Float(\"gamma\",min_value = 0 ,max_value = 5,step = 0.5)\n",
        "\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(32,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(0.1,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(128))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,128))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # LSTM LAYER\n",
        "  lstm = keras.layers.LSTM(256, return_sequences=True,input_shape =(128,))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(0.2,seed=seed_value)(lstm)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(224,3)(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(0.5,seed=seed_value)(cnn_2)\n",
        "\n",
        "  # 3ND CNN LAYER\n",
        "  cnn_3 = keras.layers.Conv1D(224,3)(lstm)\n",
        "  cnn_3 = keras.layers.Activation(activation='relu')(cnn_3)\n",
        "  cnn_3 = keras.layers.BatchNormalization()(cnn_3)\n",
        "  cnn_3 = keras.layers.Dropout(0.3,seed=seed_value)(cnn_3)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_3)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss=SparseCategoricalFocalLoss(gamma = gamma), optimizer=keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# EARLY STOPPING\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                              patience=4,\n",
        "                              restore_best_weights=True,\n",
        "                              verbose=0, mode='max')\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    focal_loss_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "RDons1NmUPg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filtered 150"
      ],
      "metadata": {
        "id": "Q4hbDn61X93V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_150['lemmatized text'] = filtered_150.text.apply(lambda x: lemmatization(x))\n",
        "\n",
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(filtered_150['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(filtered_150['lemmatized text'])\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = filtered_150['category']\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    focal_loss_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "6Xz_OC6YBVPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filtered & augmented 150"
      ],
      "metadata": {
        "id": "oAavTdJeYAac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug_train150['lemmatized text'] = aug_train150.text.apply(lambda x: lemmatization(x))\n",
        "\n",
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(aug_train150['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(aug_train150['lemmatized text'])\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = aug_train150['category']\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    focal_loss_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "Y405mlZPWEnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filtered 312"
      ],
      "metadata": {
        "id": "wkuGhT1TYFm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_312['lemmatized text'] = filtered_150.text.apply(lambda x: lemmatization(x))\n",
        "\n",
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(filtered_312['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(filtered_312['lemmatized text'])\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = filtered_312['category']\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    focal_loss_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "Pi8f0EdyWHDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filtered & augmented 312"
      ],
      "metadata": {
        "id": "0RFH7XzIYH4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug_train312['lemmatized text'] = aug_train312.text.apply(lambda x: lemmatization(x))\n",
        "\n",
        "# MAX_FEATURES = 6000\n",
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(aug_train312['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(aug_train312['lemmatized text'])\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = aug_train312['category']\n",
        "\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "LOG_DIR = f\"{int(time.time())}\"\n",
        "seed_value= 0\n",
        "\n",
        "# PARAMETER SEARCH\n",
        "tuner = RandomSearch(\n",
        "    focal_loss_model,\n",
        "    objective = keras_tuner.Objective('val_accuracy', direction=\"max\"),\n",
        "    max_trials = 10,\n",
        "    executions_per_trial = 1,\n",
        "    directory = LOG_DIR\n",
        "    )\n",
        "\n",
        "tuner.search(x=X_train1,y = y_train1,epochs = 20, batch_size = 8, validation_data = (X_test1,y_test1),callbacks = [stop_early])\n",
        "\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "LHI486CaWL8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final evaluation - Classification report"
      ],
      "metadata": {
        "id": "2xCygIsH4Iq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_num_dims = 300\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['lemmatized text'])\n",
        "index_of_words = tokenizer.word_index\n",
        "vocab_size = len(index_of_words) + 1\n",
        "\n",
        "tokenized_train = tokenizer.texts_to_sequences(train_data['lemmatized text'])\n",
        "tokenized_test = tokenizer.texts_to_sequences(test_data['lemmatized text'])\n",
        "\n",
        "\n",
        "RNN_CELL_SIZE = 32\n",
        "\n",
        "MAX_LEN = 130\n",
        "X_train1 = pad_sequences(tokenized_train, maxlen=MAX_LEN,padding='pre')\n",
        "y_train1 = train_data['category']\n",
        "\n",
        "X_test1 = pad_sequences(tokenized_test, maxlen=MAX_LEN,padding='pre')\n",
        "y_test1 = test_data['category']\n",
        "\n",
        "embedd_matrix = create_embedding_matrix(index_of_words, embed_num_dims)\n",
        "\n",
        "def final_fusion_model():\n",
        "\n",
        "  reproduceResult()\n",
        "\n",
        "  # INPUT DEFINITION\n",
        "  seq_input = keras.layers.Input(shape=(MAX_LEN,))\n",
        "\n",
        "  # EMBEDDING\n",
        "  embedded = keras.layers.Embedding(vocab_size,\n",
        "                          embed_num_dims,\n",
        "                          input_length = MAX_LEN,\n",
        "                          weights = [embedd_matrix])(seq_input)\n",
        "\n",
        "  # 1ST CNN LAYER\n",
        "  cnn_1 = keras.layers.Conv1D(32,3)(embedded)\n",
        "  cnn_1 = keras.layers.Activation(activation='relu')(cnn_1)\n",
        "  cnn_1 = keras.layers.BatchNormalization()(cnn_1)\n",
        "  cnn_1 = keras.layers.Dropout(0.1,seed=seed_value)(cnn_1)\n",
        "\n",
        "  # ATTENTION\n",
        "  attention = keras.layers.TimeDistributed(keras.layers.Dense(128))(cnn_1)\n",
        "  attention = keras.layers.Reshape((128,128))(attention)\n",
        "  attention = keras.layers.Activation('relu', name = 'cnn1_attention')(attention)\n",
        "  attention_output = keras.layers.Dot(axes = 1)([cnn_1, attention])\n",
        "\n",
        "  # LSTM LAYER\n",
        "  lstm = keras.layers.LSTM(256, return_sequences=True,input_shape =(128,))(attention_output)\n",
        "  lstm = keras.layers.Activation(activation='relu')(lstm)\n",
        "  lstm = keras.layers.BatchNormalization()(lstm)\n",
        "  lstm = keras.layers.Dropout(0.2,seed=seed_value)(lstm)\n",
        "\n",
        "  # 2ND CNN LAYER\n",
        "  cnn_2 = keras.layers.Conv1D(224,3)(lstm)\n",
        "  cnn_2 = keras.layers.Activation(activation='relu')(cnn_2)\n",
        "  cnn_2 = keras.layers.BatchNormalization()(cnn_2)\n",
        "  cnn_2 = keras.layers.Dropout(0.5,seed=seed_value)(cnn_2)\n",
        "\n",
        "  # 3ND CNN LAYER\n",
        "  cnn_3 = keras.layers.Conv1D(224,3)(lstm)\n",
        "  cnn_3 = keras.layers.Activation(activation='relu')(cnn_3)\n",
        "  cnn_3 = keras.layers.BatchNormalization()(cnn_3)\n",
        "  cnn_3 = keras.layers.Dropout(0.3,seed=seed_value)(cnn_3)\n",
        "\n",
        "  max_pooling = keras.layers.GlobalMaxPooling1D()(cnn_3)\n",
        "  output = keras.layers.Dense(6, activation='softmax')(max_pooling)\n",
        "\n",
        "  model = keras.Model(inputs = [seq_input], outputs = output)\n",
        "  model.compile(loss=SparseCategoricalFocalLoss(gamma = 3), optimizer=keras.optimizers.Adam(learning_rate=0.00024), metrics=['accuracy'])\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "9v58CU11QC9Y"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_fusion_model = final_fusion_model()\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the filepath to save the model with the HIGHEST accuracy\n",
        "filepath = 'best_model.h5'\n",
        "\n",
        "# Create the ModelCheckpoint callback\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "\n",
        "final_fusion_model.fit(X_train1, y_train1, epochs=50, batch_size=8, validation_data=(X_test1, y_test1), callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "gjLTiqNkXGEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load the model from the saved filepath\n",
        "final_fusionmodel = load_model('best_model.h5')\n",
        "\n",
        "# Get total predictions for the test data\n",
        "predictions = final_fusionmodel.predict(X_test1)\n",
        "\n",
        "predicted_labels = predictions.argmax(axis=1)"
      ],
      "metadata": {
        "id": "J4kik4psYFJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels"
      ],
      "metadata": {
        "id": "urh5lzb5YeHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "true_labels = np.array(y_test1, dtype = int)\n",
        "\n",
        "confusion = confusion_matrix(true_labels, predicted_labels)\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion)\n",
        "\n",
        "label_names = ['Obligation', 'Omission','Permission', 'Power', 'Prohibition', 'Right']\n",
        "print(classification_report(true_labels, predicted_labels, target_names=label_names))"
      ],
      "metadata": {
        "id": "oTdrSenhAx2T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}